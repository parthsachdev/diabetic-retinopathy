{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('new/new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "immatrix = []\n",
    "imlabels = []\n",
    "rows,cols = 224,224\n",
    "img_list = os.listdir('new/compress6000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:42<00:00, 151.73it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(img_list):\n",
    "    imlabels.append(list(train_labels.loc[train_labels.image==file[:-5], :].values[0]))\n",
    "    img = cv2.imread(\"new/compress6000/\" + file)   \n",
    "    img = cv2.resize(img,(rows,cols))\n",
    "    immatrix.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('compress.npy', np.asarray(immatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data,Label = shuffle(immatrix,imlabels, random_state=2)\n",
    "# train_data = [data,Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = data,Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (X,y) = (train_data[0],train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(X), np.asarray(y), test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:500] /= 255\n",
    "X_test[:500] /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[500:1000] /= 255\n",
    "X_test[500:1000] /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[1000:2000] /= 255\n",
    "X_test[1000:2000] /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[2000:4000] /= 255\n",
    "X_test[2000:4000] /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[4000:] /= 255\n",
    "X_test[4000:] /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Normalizing Data Between [0,1]\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "# print('X_train shape:', X_train.shape)\n",
    "# print(X_train.shape[0], 'train samples')\n",
    "# print(X_test.shape[0], 'test samples')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '1', '1', ..., '2', '3', '1'], dtype='<U11')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.T[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '2', ..., '1', '2', '2'], dtype='<U11')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.T[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "n_classes = 5\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train.T[2], n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test.T[2], n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5176, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54034432/58889256 [==========================>...] - ETA: 15:2 - ETA: 22:4 - ETA: 11:5 - ETA: 6:1 - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 59 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 59 - ETA: 56 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 57 - ETA: 55 - ETA: 57 - ETA: 57 - ETA: 55 - ETA: 56 - ETA: 54 - ETA: 55 - ETA: 54 - ETA: 55 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 53 - ETA: 51 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA: 4s58892288/58889256 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 52s 1us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "base_model = VGG16(weights='imagenet',include_top=False, input_shape=(224,224,3))\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten , Dropout, Activation\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model=Model(inputs=base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create generators  - training data will be augmented images\n",
    "validationdatagenerator = ImageDataGenerator()\n",
    "traindatagenerator = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1,rotation_range=15,zoom_range=0.1 )\n",
    "\n",
    "batchsize=8\n",
    "train_generator=traindatagenerator.flow(X_train, Y_train, batch_size=batchsize) \n",
    "validation_generator=validationdatagenerator.flow(X_test, Y_test,batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 5)                 6424069   \n",
      "=================================================================\n",
      "Total params: 21,138,757\n",
      "Trainable params: 13,503,493\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "     optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "     metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "nb_epochs =10\n",
    "nb_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4424/4424 [==============================] - ETA: 1:21:27 - loss: 1.8127 - acc: 0.21 - ETA: 40:52 - loss: 1.8205 - acc: 0.2188 - ETA: 27:20 - loss: 1.8389 - acc: 0.20 - ETA: 20:33 - loss: 1.8584 - acc: 0.17 - ETA: 16:29 - loss: 1.8973 - acc: 0.18 - ETA: 13:46 - loss: 1.8678 - acc: 0.19 - ETA: 11:50 - loss: 1.8515 - acc: 0.22 - ETA: 10:22 - loss: 1.8008 - acc: 0.25 - ETA: 9:14 - loss: 1.7679 - acc: 0.2743 - ETA: 8:20 - loss: 1.7384 - acc: 0.293 - ETA: 7:35 - loss: 1.6909 - acc: 0.309 - ETA: 6:58 - loss: 1.6499 - acc: 0.333 - ETA: 6:26 - loss: 1.6382 - acc: 0.351 - ETA: 5:59 - loss: 1.5933 - acc: 0.379 - ETA: 5:35 - loss: 1.5404 - acc: 0.408 - ETA: 5:15 - loss: 1.5385 - acc: 0.419 - ETA: 4:56 - loss: 1.5001 - acc: 0.439 - ETA: 4:40 - loss: 1.4750 - acc: 0.454 - ETA: 4:26 - loss: 1.4553 - acc: 0.467 - ETA: 4:12 - loss: 1.4311 - acc: 0.482 - ETA: 4:00 - loss: 1.4102 - acc: 0.494 - ETA: 3:49 - loss: 1.3926 - acc: 0.502 - ETA: 3:39 - loss: 1.3963 - acc: 0.508 - ETA: 3:30 - loss: 1.3974 - acc: 0.511 - ETA: 3:22 - loss: 1.3703 - acc: 0.523 - ETA: 3:14 - loss: 1.3522 - acc: 0.532 - ETA: 3:07 - loss: 1.3318 - acc: 0.541 - ETA: 3:00 - loss: 1.3256 - acc: 0.548 - ETA: 2:53 - loss: 1.3355 - acc: 0.548 - ETA: 2:47 - loss: 1.3083 - acc: 0.560 - ETA: 2:42 - loss: 1.3089 - acc: 0.562 - ETA: 2:37 - loss: 1.2873 - acc: 0.572 - ETA: 2:32 - loss: 1.2749 - acc: 0.578 - ETA: 2:27 - loss: 1.2681 - acc: 0.583 - ETA: 2:22 - loss: 1.2688 - acc: 0.584 - ETA: 2:18 - loss: 1.2547 - acc: 0.591 - ETA: 2:14 - loss: 1.2396 - acc: 0.596 - ETA: 2:10 - loss: 1.2236 - acc: 0.603 - ETA: 2:07 - loss: 1.2140 - acc: 0.608 - ETA: 2:03 - loss: 1.2064 - acc: 0.612 - ETA: 2:00 - loss: 1.2113 - acc: 0.611 - ETA: 1:56 - loss: 1.2060 - acc: 0.612 - ETA: 1:53 - loss: 1.1977 - acc: 0.616 - ETA: 1:50 - loss: 1.1914 - acc: 0.617 - ETA: 1:48 - loss: 1.1939 - acc: 0.617 - ETA: 1:45 - loss: 1.1885 - acc: 0.618 - ETA: 1:42 - loss: 1.1769 - acc: 0.623 - ETA: 1:40 - loss: 1.1672 - acc: 0.627 - ETA: 1:37 - loss: 1.1616 - acc: 0.629 - ETA: 1:35 - loss: 1.1557 - acc: 0.630 - ETA: 1:33 - loss: 1.1572 - acc: 0.630 - ETA: 1:30 - loss: 1.1546 - acc: 0.631 - ETA: 1:28 - loss: 1.1492 - acc: 0.633 - ETA: 1:26 - loss: 1.1437 - acc: 0.634 - ETA: 1:24 - loss: 1.1369 - acc: 0.638 - ETA: 1:22 - loss: 1.1379 - acc: 0.639 - ETA: 1:20 - loss: 1.1349 - acc: 0.640 - ETA: 1:18 - loss: 1.1289 - acc: 0.642 - ETA: 1:17 - loss: 1.1255 - acc: 0.644 - ETA: 1:15 - loss: 1.1211 - acc: 0.645 - ETA: 1:13 - loss: 1.1207 - acc: 0.647 - ETA: 1:12 - loss: 1.1189 - acc: 0.647 - ETA: 1:10 - loss: 1.1134 - acc: 0.650 - ETA: 1:08 - loss: 1.1110 - acc: 0.650 - ETA: 1:07 - loss: 1.1081 - acc: 0.651 - ETA: 1:05 - loss: 1.0980 - acc: 0.655 - ETA: 1:04 - loss: 1.0957 - acc: 0.656 - ETA: 1:02 - loss: 1.0961 - acc: 0.657 - ETA: 1:01 - loss: 1.0936 - acc: 0.658 - ETA: 1:00 - loss: 1.0860 - acc: 0.661 - ETA: 58s - loss: 1.0818 - acc: 0.662 - ETA: 57s - loss: 1.0830 - acc: 0.66 - ETA: 56s - loss: 1.0813 - acc: 0.66 - ETA: 54s - loss: 1.0815 - acc: 0.66 - ETA: 53s - loss: 1.0799 - acc: 0.66 - ETA: 52s - loss: 1.0805 - acc: 0.66 - ETA: 51s - loss: 1.0815 - acc: 0.66 - ETA: 49s - loss: 1.0778 - acc: 0.66 - ETA: 48s - loss: 1.0759 - acc: 0.66 - ETA: 47s - loss: 1.0766 - acc: 0.66 - ETA: 46s - loss: 1.0797 - acc: 0.66 - ETA: 45s - loss: 1.0755 - acc: 0.66 - ETA: 44s - loss: 1.0793 - acc: 0.66 - ETA: 43s - loss: 1.0827 - acc: 0.66 - ETA: 42s - loss: 1.0820 - acc: 0.66 - ETA: 41s - loss: 1.0775 - acc: 0.66 - ETA: 40s - loss: 1.0801 - acc: 0.66 - ETA: 39s - loss: 1.0774 - acc: 0.66 - ETA: 38s - loss: 1.0740 - acc: 0.66 - ETA: 37s - loss: 1.0707 - acc: 0.66 - ETA: 36s - loss: 1.0672 - acc: 0.66 - ETA: 35s - loss: 1.0652 - acc: 0.66 - ETA: 34s - loss: 1.0607 - acc: 0.67 - ETA: 33s - loss: 1.0617 - acc: 0.67 - ETA: 32s - loss: 1.0574 - acc: 0.67 - ETA: 31s - loss: 1.0605 - acc: 0.67 - ETA: 30s - loss: 1.0586 - acc: 0.67 - ETA: 29s - loss: 1.0579 - acc: 0.67 - ETA: 28s - loss: 1.0581 - acc: 0.67 - ETA: 27s - loss: 1.0622 - acc: 0.67 - ETA: 27s - loss: 1.0615 - acc: 0.67 - ETA: 26s - loss: 1.0574 - acc: 0.67 - ETA: 25s - loss: 1.0561 - acc: 0.67 - ETA: 24s - loss: 1.0526 - acc: 0.67 - ETA: 23s - loss: 1.0518 - acc: 0.67 - ETA: 22s - loss: 1.0510 - acc: 0.67 - ETA: 22s - loss: 1.0480 - acc: 0.67 - ETA: 21s - loss: 1.0449 - acc: 0.67 - ETA: 20s - loss: 1.0434 - acc: 0.67 - ETA: 19s - loss: 1.0404 - acc: 0.67 - ETA: 18s - loss: 1.0398 - acc: 0.67 - ETA: 18s - loss: 1.0409 - acc: 0.67 - ETA: 17s - loss: 1.0374 - acc: 0.67 - ETA: 16s - loss: 1.0383 - acc: 0.67 - ETA: 15s - loss: 1.0368 - acc: 0.67 - ETA: 15s - loss: 1.0362 - acc: 0.67 - ETA: 14s - loss: 1.0363 - acc: 0.67 - ETA: 13s - loss: 1.0357 - acc: 0.67 - ETA: 12s - loss: 1.0354 - acc: 0.67 - ETA: 12s - loss: 1.0319 - acc: 0.67 - ETA: 11s - loss: 1.0320 - acc: 0.67 - ETA: 10s - loss: 1.0339 - acc: 0.67 - ETA: 10s - loss: 1.0326 - acc: 0.67 - ETA: 9s - loss: 1.0327 - acc: 0.6794 - ETA: 8s - loss: 1.0313 - acc: 0.680 - ETA: 8s - loss: 1.0303 - acc: 0.680 - ETA: 7s - loss: 1.0277 - acc: 0.680 - ETA: 6s - loss: 1.0264 - acc: 0.681 - ETA: 6s - loss: 1.0265 - acc: 0.681 - ETA: 5s - loss: 1.0231 - acc: 0.682 - ETA: 4s - loss: 1.0235 - acc: 0.682 - ETA: 4s - loss: 1.0217 - acc: 0.682 - ETA: 3s - loss: 1.0217 - acc: 0.682 - ETA: 2s - loss: 1.0208 - acc: 0.683 - ETA: 2s - loss: 1.0203 - acc: 0.683 - ETA: 1s - loss: 1.0184 - acc: 0.684 - ETA: 0s - loss: 1.0165 - acc: 0.685 - ETA: 0s - loss: 1.0158 - acc: 0.685 - 89s 20ms/step - loss: 1.0170 - acc: 0.6844\n"
     ]
    }
   ],
   "source": [
    "model.fit(x = X_train,y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/647 [========>.....................] - ETA: 3:32:33 - loss: 1.6505 - acc: 0.12 - ETA: 2:27:46 - loss: 1.5610 - acc: 0.25 - ETA: 2:05:15 - loss: 2.4658 - acc: 0.25 - ETA: 1:54:23 - loss: 2.1757 - acc: 0.31 - ETA: 1:47:31 - loss: 2.1486 - acc: 0.25 - ETA: 1:42:56 - loss: 2.0478 - acc: 0.22 - ETA: 1:39:59 - loss: 1.9840 - acc: 0.23 - ETA: 1:37:18 - loss: 1.9293 - acc: 0.26 - ETA: 1:35:17 - loss: 1.8370 - acc: 0.30 - ETA: 1:33:38 - loss: 1.8479 - acc: 0.30 - ETA: 1:32:17 - loss: 1.8360 - acc: 0.29 - ETA: 1:31:10 - loss: 1.8251 - acc: 0.29 - ETA: 1:30:10 - loss: 1.7988 - acc: 0.29 - ETA: 1:29:17 - loss: 1.7686 - acc: 0.31 - ETA: 1:28:33 - loss: 1.7590 - acc: 0.31 - ETA: 1:27:52 - loss: 1.7445 - acc: 0.32 - ETA: 1:27:13 - loss: 1.7275 - acc: 0.32 - ETA: 1:26:40 - loss: 1.7212 - acc: 0.31 - ETA: 1:26:07 - loss: 1.6999 - acc: 0.33 - ETA: 1:25:39 - loss: 1.6935 - acc: 0.33 - ETA: 1:25:11 - loss: 1.6935 - acc: 0.33 - ETA: 1:24:47 - loss: 1.6722 - acc: 0.34 - ETA: 1:24:21 - loss: 1.6580 - acc: 0.34 - ETA: 1:23:58 - loss: 1.6453 - acc: 0.34 - ETA: 1:23:36 - loss: 1.6275 - acc: 0.36 - ETA: 1:23:15 - loss: 1.6096 - acc: 0.36 - ETA: 1:22:56 - loss: 1.6156 - acc: 0.36 - ETA: 1:22:37 - loss: 1.5980 - acc: 0.37 - ETA: 1:22:19 - loss: 1.5855 - acc: 0.37 - ETA: 1:22:02 - loss: 1.5955 - acc: 0.37 - ETA: 1:21:47 - loss: 1.5881 - acc: 0.38 - ETA: 1:21:31 - loss: 1.5818 - acc: 0.37 - ETA: 1:21:16 - loss: 1.5767 - acc: 0.37 - ETA: 1:21:01 - loss: 1.5775 - acc: 0.37 - ETA: 1:20:46 - loss: 1.5731 - acc: 0.36 - ETA: 1:20:32 - loss: 1.5755 - acc: 0.36 - ETA: 1:20:18 - loss: 1.5681 - acc: 0.37 - ETA: 1:20:05 - loss: 1.5758 - acc: 0.36 - ETA: 1:19:52 - loss: 1.5715 - acc: 0.37 - ETA: 1:19:39 - loss: 1.5684 - acc: 0.37 - ETA: 1:19:26 - loss: 1.5656 - acc: 0.37 - ETA: 1:19:14 - loss: 1.5706 - acc: 0.37 - ETA: 1:19:02 - loss: 1.5659 - acc: 0.36 - ETA: 1:18:50 - loss: 1.5719 - acc: 0.36 - ETA: 1:18:39 - loss: 1.5666 - acc: 0.36 - ETA: 1:18:27 - loss: 1.5685 - acc: 0.36 - ETA: 1:18:19 - loss: 1.5670 - acc: 0.36 - ETA: 1:18:08 - loss: 1.5713 - acc: 0.36 - ETA: 1:17:57 - loss: 1.5693 - acc: 0.36 - ETA: 1:17:46 - loss: 1.5730 - acc: 0.36 - ETA: 1:17:35 - loss: 1.5710 - acc: 0.36 - ETA: 1:17:24 - loss: 1.5753 - acc: 0.35 - ETA: 1:17:13 - loss: 1.5744 - acc: 0.35 - ETA: 1:17:04 - loss: 1.5757 - acc: 0.34 - ETA: 1:16:53 - loss: 1.5736 - acc: 0.35 - ETA: 1:16:43 - loss: 1.5708 - acc: 0.35 - ETA: 1:16:32 - loss: 1.5678 - acc: 0.35 - ETA: 1:16:21 - loss: 1.5615 - acc: 0.36 - ETA: 1:16:11 - loss: 1.5569 - acc: 0.36 - ETA: 1:16:00 - loss: 1.5536 - acc: 0.36 - ETA: 1:15:50 - loss: 1.5515 - acc: 0.37 - ETA: 1:15:40 - loss: 1.5579 - acc: 0.36 - ETA: 1:15:31 - loss: 1.5583 - acc: 0.36 - ETA: 1:15:21 - loss: 1.5511 - acc: 0.37 - ETA: 1:15:17 - loss: 1.5422 - acc: 0.37 - ETA: 1:15:09 - loss: 1.5372 - acc: 0.38 - ETA: 1:15:00 - loss: 1.5392 - acc: 0.38 - ETA: 1:14:50 - loss: 1.5413 - acc: 0.38 - ETA: 1:14:41 - loss: 1.5391 - acc: 0.38 - ETA: 1:14:35 - loss: 1.5389 - acc: 0.38 - ETA: 1:14:33 - loss: 1.5375 - acc: 0.38 - ETA: 1:14:26 - loss: 1.5321 - acc: 0.38 - ETA: 1:14:17 - loss: 1.5427 - acc: 0.38 - ETA: 1:14:08 - loss: 1.5404 - acc: 0.38 - ETA: 1:13:58 - loss: 1.5402 - acc: 0.38 - ETA: 1:13:49 - loss: 1.5430 - acc: 0.38 - ETA: 1:13:40 - loss: 1.5419 - acc: 0.38 - ETA: 1:13:33 - loss: 1.5407 - acc: 0.38 - ETA: 1:13:24 - loss: 1.5398 - acc: 0.39 - ETA: 1:13:15 - loss: 1.5442 - acc: 0.38 - ETA: 1:13:08 - loss: 1.5428 - acc: 0.38 - ETA: 1:13:05 - loss: 1.5412 - acc: 0.38 - ETA: 1:13:11 - loss: 1.5395 - acc: 0.39 - ETA: 1:13:11 - loss: 1.5402 - acc: 0.39 - ETA: 1:13:04 - loss: 1.5426 - acc: 0.38 - ETA: 1:13:04 - loss: 1.5421 - acc: 0.38 - ETA: 1:13:00 - loss: 1.5419 - acc: 0.38 - ETA: 1:12:51 - loss: 1.5419 - acc: 0.39 - ETA: 1:12:41 - loss: 1.5418 - acc: 0.38 - ETA: 1:12:32 - loss: 1.5377 - acc: 0.39 - ETA: 1:12:23 - loss: 1.5396 - acc: 0.39 - ETA: 1:12:13 - loss: 1.5425 - acc: 0.38 - ETA: 1:12:05 - loss: 1.5407 - acc: 0.39 - ETA: 1:11:56 - loss: 1.5369 - acc: 0.39 - ETA: 1:11:47 - loss: 1.5335 - acc: 0.39 - ETA: 1:11:38 - loss: 1.5406 - acc: 0.39 - ETA: 1:11:29 - loss: 1.5425 - acc: 0.39 - ETA: 1:11:19 - loss: 1.5406 - acc: 0.39 - ETA: 1:11:10 - loss: 1.5398 - acc: 0.39 - ETA: 1:11:01 - loss: 1.5415 - acc: 0.39 - ETA: 1:10:53 - loss: 1.5404 - acc: 0.39 - ETA: 1:10:43 - loss: 1.5349 - acc: 0.39 - ETA: 1:10:34 - loss: 1.5373 - acc: 0.39 - ETA: 1:10:25 - loss: 1.5374 - acc: 0.39 - ETA: 1:10:16 - loss: 1.5396 - acc: 0.39 - ETA: 1:10:07 - loss: 1.5402 - acc: 0.39 - ETA: 1:09:58 - loss: 1.5389 - acc: 0.39 - ETA: 1:09:48 - loss: 1.5380 - acc: 0.39 - ETA: 1:09:39 - loss: 1.5352 - acc: 0.39 - ETA: 1:09:30 - loss: 1.5341 - acc: 0.40 - ETA: 1:09:22 - loss: 1.5331 - acc: 0.39 - ETA: 1:09:14 - loss: 1.5323 - acc: 0.39 - ETA: 1:09:05 - loss: 1.5362 - acc: 0.39 - ETA: 1:08:56 - loss: 1.5355 - acc: 0.39 - ETA: 1:08:47 - loss: 1.5342 - acc: 0.40 - ETA: 1:08:40 - loss: 1.5353 - acc: 0.39 - ETA: 1:08:32 - loss: 1.5363 - acc: 0.39 - ETA: 1:08:24 - loss: 1.5367 - acc: 0.39 - ETA: 1:08:15 - loss: 1.5352 - acc: 0.39 - ETA: 1:08:06 - loss: 1.5339 - acc: 0.39 - ETA: 1:07:57 - loss: 1.5333 - acc: 0.39 - ETA: 1:07:49 - loss: 1.5308 - acc: 0.39 - ETA: 1:07:40 - loss: 1.5305 - acc: 0.39 - ETA: 1:07:31 - loss: 1.5318 - acc: 0.39 - ETA: 1:07:23 - loss: 1.5324 - acc: 0.39 - ETA: 1:07:14 - loss: 1.5324 - acc: 0.39 - ETA: 1:07:06 - loss: 1.5334 - acc: 0.39 - ETA: 1:06:57 - loss: 1.5329 - acc: 0.39 - ETA: 1:06:48 - loss: 1.5324 - acc: 0.39 - ETA: 1:06:40 - loss: 1.5342 - acc: 0.39 - ETA: 1:06:32 - loss: 1.5346 - acc: 0.39 - ETA: 1:06:23 - loss: 1.5338 - acc: 0.39 - ETA: 1:06:15 - loss: 1.5330 - acc: 0.39 - ETA: 1:06:06 - loss: 1.5315 - acc: 0.39 - ETA: 1:05:57 - loss: 1.5277 - acc: 0.39 - ETA: 1:05:49 - loss: 1.5267 - acc: 0.39 - ETA: 1:05:40 - loss: 1.5269 - acc: 0.39 - ETA: 1:05:31 - loss: 1.5275 - acc: 0.39 - ETA: 1:05:23 - loss: 1.5270 - acc: 0.39 - ETA: 1:05:14 - loss: 1.5284 - acc: 0.39 - ETA: 1:05:06 - loss: 1.5283 - acc: 0.39 - ETA: 1:04:57 - loss: 1.5291 - acc: 0.39 - ETA: 1:04:49 - loss: 1.5292 - acc: 0.39 - ETA: 1:04:40 - loss: 1.5287 - acc: 0.39 - ETA: 1:04:32 - loss: 1.5285 - acc: 0.39 - ETA: 1:04:23 - loss: 1.5269 - acc: 0.39 - ETA: 1:04:15 - loss: 1.5277 - acc: 0.39 - ETA: 1:04:07 - loss: 1.5301 - acc: 0.39 - ETA: 1:03:58 - loss: 1.5310 - acc: 0.39 - ETA: 1:03:50 - loss: 1.5313 - acc: 0.38 - ETA: 1:03:43 - loss: 1.5309 - acc: 0.38 - ETA: 1:03:35 - loss: 1.5301 - acc: 0.38 - ETA: 1:03:26 - loss: 1.5302 - acc: 0.38 - ETA: 1:03:18 - loss: 1.5308 - acc: 0.38 - ETA: 1:03:10 - loss: 1.5303 - acc: 0.38 - ETA: 1:03:02 - loss: 1.5297 - acc: 0.38 - ETA: 1:02:55 - loss: 1.5281 - acc: 0.39 - ETA: 1:02:47 - loss: 1.5276 - acc: 0.39 - ETA: 1:02:40 - loss: 1.5275 - acc: 0.38 - ETA: 1:02:33 - loss: 1.5268 - acc: 0.38 - ETA: 1:02:25 - loss: 1.5280 - acc: 0.38 - ETA: 1:02:17 - loss: 1.5282 - acc: 0.38 - ETA: 1:02:09 - loss: 1.5281 - acc: 0.38 - ETA: 1:02:01 - loss: 1.5293 - acc: 0.38 - ETA: 1:01:53 - loss: 1.5293 - acc: 0.38 - ETA: 1:01:46 - loss: 1.5287 - acc: 0.38 - ETA: 1:01:39 - loss: 1.5279 - acc: 0.38 - ETA: 1:01:31 - loss: 1.5279 - acc: 0.38 - ETA: 1:01:23 - loss: 1.5281 - acc: 0.38 - ETA: 1:01:14 - loss: 1.5287 - acc: 0.38 - ETA: 1:01:06 - loss: 1.5292 - acc: 0.38 - ETA: 1:00:58 - loss: 1.5297 - acc: 0.38 - ETA: 1:00:49 - loss: 1.5298 - acc: 0.38 - ETA: 1:00:41 - loss: 1.5296 - acc: 0.38 - ETA: 1:00:33 - loss: 1.5302 - acc: 0.38 - ETA: 1:00:25 - loss: 1.5307 - acc: 0.37 - ETA: 1:00:17 - loss: 1.5309 - acc: 0.37 - ETA: 1:00:10 - loss: 1.5308 - acc: 0.37 - ETA: 1:00:02 - loss: 1.5313 - acc: 0.37 - ETA: 59:54 - loss: 1.5308 - acc: 0.3771 - ETA: 59:46 - loss: 1.5307 - acc: 0.37 - ETA: 59:37 - loss: 1.5312 - acc: 0.37 - ETA: 59:29 - loss: 1.5303 - acc: 0.37 - ETA: 59:21 - loss: 1.5302 - acc: 0.37 - ETA: 59:13 - loss: 1.5306 - acc: 0.37 - ETA: 59:05 - loss: 1.5287 - acc: 0.37 - ETA: 58:56 - loss: 1.5292 - acc: 0.37 - ETA: 58:48 - loss: 1.5282 - acc: 0.38 - ETA: 58:40 - loss: 1.5287 - acc: 0.37 - ETA: 58:32 - loss: 1.5276 - acc: 0.37 - ETA: 58:24 - loss: 1.5277 - acc: 0.37 - ETA: 58:16 - loss: 1.5270 - acc: 0.38 - ETA: 58:08 - loss: 1.5265 - acc: 0.38 - ETA: 57:59 - loss: 1.5283 - acc: 0.37 - ETA: 57:51 - loss: 1.5289 - acc: 0.37399/647 [=================>............] - ETA: 57:43 - loss: 1.5280 - acc: 0.37 - ETA: 57:35 - loss: 1.5295 - acc: 0.37 - ETA: 57:27 - loss: 1.5298 - acc: 0.37 - ETA: 57:19 - loss: 1.5296 - acc: 0.37 - ETA: 57:11 - loss: 1.5289 - acc: 0.37 - ETA: 57:03 - loss: 1.5282 - acc: 0.37 - ETA: 56:55 - loss: 1.5277 - acc: 0.37 - ETA: 56:48 - loss: 1.5269 - acc: 0.37 - ETA: 56:41 - loss: 1.5266 - acc: 0.37 - ETA: 56:33 - loss: 1.5249 - acc: 0.38 - ETA: 56:25 - loss: 1.5263 - acc: 0.37 - ETA: 56:17 - loss: 1.5256 - acc: 0.38 - ETA: 56:09 - loss: 1.5250 - acc: 0.38 - ETA: 56:02 - loss: 1.5248 - acc: 0.38 - ETA: 55:54 - loss: 1.5265 - acc: 0.38 - ETA: 55:47 - loss: 1.5249 - acc: 0.38 - ETA: 55:39 - loss: 1.5233 - acc: 0.38 - ETA: 55:32 - loss: 1.5224 - acc: 0.38 - ETA: 55:24 - loss: 1.5242 - acc: 0.38 - ETA: 55:17 - loss: 1.5250 - acc: 0.38 - ETA: 55:10 - loss: 1.5239 - acc: 0.38 - ETA: 55:02 - loss: 1.5233 - acc: 0.38 - ETA: 54:54 - loss: 1.5228 - acc: 0.38 - ETA: 54:48 - loss: 1.5221 - acc: 0.38 - ETA: 54:40 - loss: 1.5207 - acc: 0.38 - ETA: 54:33 - loss: 1.5206 - acc: 0.38 - ETA: 54:25 - loss: 1.5219 - acc: 0.38 - ETA: 54:18 - loss: 1.5201 - acc: 0.38 - ETA: 54:11 - loss: 1.5200 - acc: 0.38 - ETA: 54:03 - loss: 1.5210 - acc: 0.38 - ETA: 53:56 - loss: 1.5203 - acc: 0.38 - ETA: 53:50 - loss: 1.5208 - acc: 0.38 - ETA: 53:42 - loss: 1.5219 - acc: 0.38 - ETA: 53:34 - loss: 1.5209 - acc: 0.38 - ETA: 53:26 - loss: 1.5201 - acc: 0.38 - ETA: 53:18 - loss: 1.5192 - acc: 0.38 - ETA: 53:10 - loss: 1.5185 - acc: 0.38 - ETA: 53:02 - loss: 1.5199 - acc: 0.38 - ETA: 52:54 - loss: 1.5199 - acc: 0.38 - ETA: 52:46 - loss: 1.5181 - acc: 0.38 - ETA: 52:39 - loss: 1.5190 - acc: 0.38 - ETA: 52:31 - loss: 1.5193 - acc: 0.38 - ETA: 52:23 - loss: 1.5196 - acc: 0.38 - ETA: 52:15 - loss: 1.5204 - acc: 0.38 - ETA: 52:07 - loss: 1.5201 - acc: 0.38 - ETA: 51:59 - loss: 1.5193 - acc: 0.38 - ETA: 51:51 - loss: 1.5209 - acc: 0.38 - ETA: 51:44 - loss: 1.5215 - acc: 0.38 - ETA: 51:36 - loss: 1.5215 - acc: 0.38 - ETA: 51:28 - loss: 1.5217 - acc: 0.38 - ETA: 51:20 - loss: 1.5212 - acc: 0.38 - ETA: 51:12 - loss: 1.5208 - acc: 0.38 - ETA: 51:04 - loss: 1.5202 - acc: 0.38 - ETA: 50:56 - loss: 1.5205 - acc: 0.38 - ETA: 50:48 - loss: 1.5191 - acc: 0.38 - ETA: 50:40 - loss: 1.5177 - acc: 0.38 - ETA: 50:32 - loss: 1.5170 - acc: 0.38 - ETA: 50:24 - loss: 1.5155 - acc: 0.39 - ETA: 50:16 - loss: 1.5143 - acc: 0.39 - ETA: 50:08 - loss: 1.5140 - acc: 0.39 - ETA: 50:00 - loss: 1.5128 - acc: 0.39 - ETA: 49:52 - loss: 1.5129 - acc: 0.39 - ETA: 49:44 - loss: 1.5137 - acc: 0.39 - ETA: 49:36 - loss: 1.5143 - acc: 0.39 - ETA: 49:28 - loss: 1.5143 - acc: 0.39 - ETA: 49:20 - loss: 1.5141 - acc: 0.39 - ETA: 49:12 - loss: 1.5148 - acc: 0.39 - ETA: 49:04 - loss: 1.5139 - acc: 0.39 - ETA: 48:56 - loss: 1.5122 - acc: 0.39 - ETA: 48:49 - loss: 1.5110 - acc: 0.39 - ETA: 48:41 - loss: 1.5107 - acc: 0.39 - ETA: 48:34 - loss: 1.5105 - acc: 0.39 - ETA: 48:26 - loss: 1.5114 - acc: 0.39 - ETA: 48:18 - loss: 1.5100 - acc: 0.39 - ETA: 48:10 - loss: 1.5092 - acc: 0.39 - ETA: 48:02 - loss: 1.5102 - acc: 0.39 - ETA: 47:54 - loss: 1.5113 - acc: 0.39 - ETA: 47:46 - loss: 1.5114 - acc: 0.39 - ETA: 47:39 - loss: 1.5116 - acc: 0.39 - ETA: 47:31 - loss: 1.5116 - acc: 0.39 - ETA: 47:23 - loss: 1.5122 - acc: 0.39 - ETA: 47:15 - loss: 1.5130 - acc: 0.39 - ETA: 47:07 - loss: 1.5133 - acc: 0.39 - ETA: 46:59 - loss: 1.5134 - acc: 0.39 - ETA: 46:51 - loss: 1.5135 - acc: 0.38 - ETA: 46:43 - loss: 1.5136 - acc: 0.38 - ETA: 46:36 - loss: 1.5141 - acc: 0.38 - ETA: 46:28 - loss: 1.5139 - acc: 0.38 - ETA: 46:21 - loss: 1.5138 - acc: 0.38 - ETA: 46:13 - loss: 1.5136 - acc: 0.38 - ETA: 46:05 - loss: 1.5137 - acc: 0.38 - ETA: 45:57 - loss: 1.5133 - acc: 0.38 - ETA: 45:49 - loss: 1.5139 - acc: 0.38 - ETA: 45:41 - loss: 1.5139 - acc: 0.38 - ETA: 45:34 - loss: 1.5141 - acc: 0.38 - ETA: 45:26 - loss: 1.5143 - acc: 0.38 - ETA: 45:18 - loss: 1.5145 - acc: 0.38 - ETA: 45:10 - loss: 1.5144 - acc: 0.38 - ETA: 45:02 - loss: 1.5142 - acc: 0.38 - ETA: 44:54 - loss: 1.5137 - acc: 0.38 - ETA: 44:47 - loss: 1.5138 - acc: 0.38 - ETA: 44:39 - loss: 1.5143 - acc: 0.38 - ETA: 44:31 - loss: 1.5144 - acc: 0.38 - ETA: 44:23 - loss: 1.5146 - acc: 0.38 - ETA: 44:15 - loss: 1.5146 - acc: 0.38 - ETA: 44:07 - loss: 1.5142 - acc: 0.38 - ETA: 44:00 - loss: 1.5136 - acc: 0.38 - ETA: 43:52 - loss: 1.5144 - acc: 0.38 - ETA: 43:44 - loss: 1.5145 - acc: 0.38 - ETA: 43:36 - loss: 1.5150 - acc: 0.38 - ETA: 43:28 - loss: 1.5157 - acc: 0.38 - ETA: 43:20 - loss: 1.5156 - acc: 0.38 - ETA: 43:13 - loss: 1.5155 - acc: 0.38 - ETA: 43:05 - loss: 1.5150 - acc: 0.38 - ETA: 42:57 - loss: 1.5147 - acc: 0.38 - ETA: 42:49 - loss: 1.5148 - acc: 0.38 - ETA: 42:41 - loss: 1.5149 - acc: 0.38 - ETA: 42:34 - loss: 1.5147 - acc: 0.38 - ETA: 42:26 - loss: 1.5147 - acc: 0.38 - ETA: 42:18 - loss: 1.5156 - acc: 0.38 - ETA: 42:10 - loss: 1.5157 - acc: 0.38 - ETA: 42:03 - loss: 1.5153 - acc: 0.38 - ETA: 41:55 - loss: 1.5153 - acc: 0.38 - ETA: 41:47 - loss: 1.5160 - acc: 0.38 - ETA: 41:39 - loss: 1.5161 - acc: 0.38 - ETA: 41:32 - loss: 1.5158 - acc: 0.38 - ETA: 41:24 - loss: 1.5153 - acc: 0.38 - ETA: 41:17 - loss: 1.5156 - acc: 0.38 - ETA: 41:09 - loss: 1.5152 - acc: 0.38 - ETA: 41:01 - loss: 1.5154 - acc: 0.38 - ETA: 40:53 - loss: 1.5152 - acc: 0.38 - ETA: 40:46 - loss: 1.5151 - acc: 0.38 - ETA: 40:38 - loss: 1.5149 - acc: 0.38 - ETA: 40:30 - loss: 1.5143 - acc: 0.38 - ETA: 40:22 - loss: 1.5131 - acc: 0.38 - ETA: 40:15 - loss: 1.5156 - acc: 0.38 - ETA: 40:07 - loss: 1.5154 - acc: 0.38 - ETA: 39:59 - loss: 1.5158 - acc: 0.38 - ETA: 39:51 - loss: 1.5158 - acc: 0.38 - ETA: 39:44 - loss: 1.5160 - acc: 0.38 - ETA: 39:36 - loss: 1.5162 - acc: 0.38 - ETA: 39:28 - loss: 1.5163 - acc: 0.38 - ETA: 39:21 - loss: 1.5164 - acc: 0.38 - ETA: 39:14 - loss: 1.5160 - acc: 0.38 - ETA: 39:06 - loss: 1.5155 - acc: 0.38 - ETA: 38:58 - loss: 1.5149 - acc: 0.38 - ETA: 38:51 - loss: 1.5148 - acc: 0.38 - ETA: 38:43 - loss: 1.5145 - acc: 0.38 - ETA: 38:35 - loss: 1.5140 - acc: 0.38 - ETA: 38:27 - loss: 1.5133 - acc: 0.38 - ETA: 38:20 - loss: 1.5136 - acc: 0.38 - ETA: 38:12 - loss: 1.5132 - acc: 0.38 - ETA: 38:04 - loss: 1.5134 - acc: 0.38 - ETA: 37:56 - loss: 1.5133 - acc: 0.38 - ETA: 37:49 - loss: 1.5132 - acc: 0.38 - ETA: 37:41 - loss: 1.5133 - acc: 0.38 - ETA: 37:33 - loss: 1.5130 - acc: 0.38 - ETA: 37:26 - loss: 1.5125 - acc: 0.38 - ETA: 37:18 - loss: 1.5123 - acc: 0.38 - ETA: 37:11 - loss: 1.5120 - acc: 0.38 - ETA: 37:03 - loss: 1.5118 - acc: 0.38 - ETA: 36:55 - loss: 1.5121 - acc: 0.38 - ETA: 36:48 - loss: 1.5109 - acc: 0.38 - ETA: 36:40 - loss: 1.5110 - acc: 0.38 - ETA: 36:32 - loss: 1.5113 - acc: 0.38 - ETA: 36:24 - loss: 1.5113 - acc: 0.38 - ETA: 36:17 - loss: 1.5105 - acc: 0.38 - ETA: 36:09 - loss: 1.5101 - acc: 0.38 - ETA: 36:02 - loss: 1.5096 - acc: 0.38 - ETA: 35:54 - loss: 1.5090 - acc: 0.38 - ETA: 35:46 - loss: 1.5083 - acc: 0.38 - ETA: 35:39 - loss: 1.5074 - acc: 0.38 - ETA: 35:31 - loss: 1.5060 - acc: 0.38 - ETA: 35:23 - loss: 1.5066 - acc: 0.38 - ETA: 35:16 - loss: 1.5071 - acc: 0.38 - ETA: 35:08 - loss: 1.5064 - acc: 0.38 - ETA: 35:00 - loss: 1.5058 - acc: 0.38 - ETA: 34:52 - loss: 1.5058 - acc: 0.38 - ETA: 34:45 - loss: 1.5062 - acc: 0.38 - ETA: 34:37 - loss: 1.5064 - acc: 0.38 - ETA: 34:29 - loss: 1.5066 - acc: 0.38 - ETA: 34:21 - loss: 1.5066 - acc: 0.38 - ETA: 34:14 - loss: 1.5064 - acc: 0.38 - ETA: 34:06 - loss: 1.5056 - acc: 0.38 - ETA: 33:58 - loss: 1.5043 - acc: 0.38 - ETA: 33:50 - loss: 1.5033 - acc: 0.38 - ETA: 33:43 - loss: 1.5038 - acc: 0.38 - ETA: 33:35 - loss: 1.5035 - acc: 0.38 - ETA: 33:27 - loss: 1.5040 - acc: 0.38 - ETA: 33:20 - loss: 1.5025 - acc: 0.38 - ETA: 33:12 - loss: 1.5032 - acc: 0.38 - ETA: 33:04 - loss: 1.5035 - acc: 0.38 - ETA: 32:56 - loss: 1.5031 - acc: 0.38 - ETA: 32:49 - loss: 1.5030 - acc: 0.38 - ETA: 32:41 - loss: 1.5033 - acc: 0.38 - ETA: 32:34 - loss: 1.5035 - acc: 0.38 - ETA: 32:26 - loss: 1.5033 - acc: 0.38 - ETA: 32:18 - loss: 1.5034 - acc: 0.38 - ETA: 32:10 - loss: 1.5031 - acc: 0.38 - ETA: 32:03 - loss: 1.5027 - acc: 0.38 - ETA: 31:55 - loss: 1.5029 - acc: 0.38 - ETA: 31:47 - loss: 1.5028 - acc: 0.38 - ETA: 31:40 - loss: 1.5026 - acc: 0.38 - ETA: 31:32 - loss: 1.5031 - acc: 0.3872"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603/647 [==========================>...] - ETA: 31:24 - loss: 1.5029 - acc: 0.38 - ETA: 31:16 - loss: 1.5036 - acc: 0.38 - ETA: 31:09 - loss: 1.5035 - acc: 0.38 - ETA: 31:01 - loss: 1.5040 - acc: 0.38 - ETA: 30:54 - loss: 1.5036 - acc: 0.38 - ETA: 30:46 - loss: 1.5042 - acc: 0.38 - ETA: 30:38 - loss: 1.5040 - acc: 0.38 - ETA: 30:30 - loss: 1.5037 - acc: 0.38 - ETA: 30:23 - loss: 1.5040 - acc: 0.38 - ETA: 30:15 - loss: 1.5044 - acc: 0.38 - ETA: 30:07 - loss: 1.5043 - acc: 0.38 - ETA: 30:00 - loss: 1.5038 - acc: 0.38 - ETA: 29:52 - loss: 1.5027 - acc: 0.38 - ETA: 29:44 - loss: 1.5026 - acc: 0.38 - ETA: 29:37 - loss: 1.5031 - acc: 0.38 - ETA: 29:29 - loss: 1.5025 - acc: 0.38 - ETA: 29:21 - loss: 1.5032 - acc: 0.38 - ETA: 29:14 - loss: 1.5026 - acc: 0.38 - ETA: 29:06 - loss: 1.5037 - acc: 0.38 - ETA: 28:58 - loss: 1.5043 - acc: 0.38 - ETA: 28:51 - loss: 1.5041 - acc: 0.38 - ETA: 28:43 - loss: 1.5035 - acc: 0.38 - ETA: 28:35 - loss: 1.5030 - acc: 0.38 - ETA: 28:28 - loss: 1.5028 - acc: 0.38 - ETA: 28:20 - loss: 1.5016 - acc: 0.38 - ETA: 28:12 - loss: 1.5018 - acc: 0.38 - ETA: 28:04 - loss: 1.5025 - acc: 0.38 - ETA: 27:57 - loss: 1.5030 - acc: 0.38 - ETA: 27:49 - loss: 1.5029 - acc: 0.38 - ETA: 27:41 - loss: 1.5028 - acc: 0.38 - ETA: 27:34 - loss: 1.5032 - acc: 0.38 - ETA: 27:26 - loss: 1.5036 - acc: 0.38 - ETA: 27:18 - loss: 1.5036 - acc: 0.38 - ETA: 27:11 - loss: 1.5040 - acc: 0.38 - ETA: 27:03 - loss: 1.5039 - acc: 0.38 - ETA: 26:55 - loss: 1.5038 - acc: 0.38 - ETA: 26:48 - loss: 1.5037 - acc: 0.38 - ETA: 26:40 - loss: 1.5039 - acc: 0.38 - ETA: 26:32 - loss: 1.5035 - acc: 0.38 - ETA: 26:24 - loss: 1.5031 - acc: 0.38 - ETA: 26:17 - loss: 1.5025 - acc: 0.39 - ETA: 26:09 - loss: 1.5027 - acc: 0.39 - ETA: 26:01 - loss: 1.5021 - acc: 0.39 - ETA: 25:54 - loss: 1.5017 - acc: 0.39 - ETA: 25:46 - loss: 1.5013 - acc: 0.39 - ETA: 25:38 - loss: 1.5020 - acc: 0.39 - ETA: 25:31 - loss: 1.5015 - acc: 0.39 - ETA: 25:23 - loss: 1.5019 - acc: 0.39 - ETA: 25:15 - loss: 1.5021 - acc: 0.39 - ETA: 25:08 - loss: 1.5013 - acc: 0.39 - ETA: 25:00 - loss: 1.5006 - acc: 0.39 - ETA: 24:52 - loss: 1.5017 - acc: 0.39 - ETA: 24:45 - loss: 1.5017 - acc: 0.39 - ETA: 24:37 - loss: 1.5018 - acc: 0.39 - ETA: 24:30 - loss: 1.5010 - acc: 0.39 - ETA: 24:22 - loss: 1.5015 - acc: 0.39 - ETA: 24:14 - loss: 1.5009 - acc: 0.39 - ETA: 24:07 - loss: 1.5006 - acc: 0.39 - ETA: 23:59 - loss: 1.5005 - acc: 0.39 - ETA: 23:51 - loss: 1.5006 - acc: 0.39 - ETA: 23:44 - loss: 1.4996 - acc: 0.39 - ETA: 23:36 - loss: 1.4999 - acc: 0.39 - ETA: 23:28 - loss: 1.4999 - acc: 0.39 - ETA: 23:21 - loss: 1.5001 - acc: 0.39 - ETA: 23:13 - loss: 1.5000 - acc: 0.39 - ETA: 23:05 - loss: 1.4999 - acc: 0.39 - ETA: 22:58 - loss: 1.5006 - acc: 0.39 - ETA: 22:50 - loss: 1.5006 - acc: 0.39 - ETA: 22:42 - loss: 1.5005 - acc: 0.39 - ETA: 22:35 - loss: 1.5005 - acc: 0.39 - ETA: 22:27 - loss: 1.5003 - acc: 0.39 - ETA: 22:19 - loss: 1.4998 - acc: 0.39 - ETA: 22:12 - loss: 1.4993 - acc: 0.39 - ETA: 22:04 - loss: 1.4983 - acc: 0.39 - ETA: 21:56 - loss: 1.4981 - acc: 0.39 - ETA: 21:49 - loss: 1.4984 - acc: 0.39 - ETA: 21:41 - loss: 1.4980 - acc: 0.39 - ETA: 21:33 - loss: 1.4973 - acc: 0.39 - ETA: 21:26 - loss: 1.4972 - acc: 0.39 - ETA: 21:18 - loss: 1.4964 - acc: 0.39 - ETA: 21:11 - loss: 1.4960 - acc: 0.39 - ETA: 21:03 - loss: 1.4961 - acc: 0.39 - ETA: 20:56 - loss: 1.4961 - acc: 0.39 - ETA: 20:48 - loss: 1.4963 - acc: 0.39 - ETA: 20:40 - loss: 1.4962 - acc: 0.39 - ETA: 20:33 - loss: 1.4954 - acc: 0.39 - ETA: 20:25 - loss: 1.4951 - acc: 0.39 - ETA: 20:18 - loss: 1.4963 - acc: 0.39 - ETA: 20:10 - loss: 1.4961 - acc: 0.39 - ETA: 20:03 - loss: 1.4963 - acc: 0.39 - ETA: 19:55 - loss: 1.4969 - acc: 0.39 - ETA: 19:47 - loss: 1.4967 - acc: 0.39 - ETA: 19:40 - loss: 1.4965 - acc: 0.39 - ETA: 19:32 - loss: 1.4966 - acc: 0.39 - ETA: 19:24 - loss: 1.4967 - acc: 0.39 - ETA: 19:17 - loss: 1.4964 - acc: 0.39 - ETA: 19:09 - loss: 1.4958 - acc: 0.39 - ETA: 19:02 - loss: 1.4957 - acc: 0.39 - ETA: 18:54 - loss: 1.4944 - acc: 0.39 - ETA: 18:46 - loss: 1.4946 - acc: 0.39 - ETA: 18:39 - loss: 1.4941 - acc: 0.39 - ETA: 18:31 - loss: 1.4942 - acc: 0.39 - ETA: 18:23 - loss: 1.4943 - acc: 0.39 - ETA: 18:16 - loss: 1.4944 - acc: 0.39 - ETA: 18:08 - loss: 1.4938 - acc: 0.39 - ETA: 18:00 - loss: 1.4940 - acc: 0.39 - ETA: 17:53 - loss: 1.4944 - acc: 0.39 - ETA: 17:45 - loss: 1.4942 - acc: 0.39 - ETA: 17:38 - loss: 1.4941 - acc: 0.39 - ETA: 17:30 - loss: 1.4938 - acc: 0.39 - ETA: 17:22 - loss: 1.4941 - acc: 0.39 - ETA: 17:15 - loss: 1.4937 - acc: 0.39 - ETA: 17:07 - loss: 1.4943 - acc: 0.39 - ETA: 16:59 - loss: 1.4942 - acc: 0.39 - ETA: 16:52 - loss: 1.4937 - acc: 0.39 - ETA: 16:44 - loss: 1.4930 - acc: 0.39 - ETA: 16:36 - loss: 1.4920 - acc: 0.39 - ETA: 16:29 - loss: 1.4934 - acc: 0.39 - ETA: 16:21 - loss: 1.4935 - acc: 0.39 - ETA: 16:13 - loss: 1.4931 - acc: 0.39 - ETA: 16:06 - loss: 1.4924 - acc: 0.39 - ETA: 15:58 - loss: 1.4922 - acc: 0.39 - ETA: 15:51 - loss: 1.4927 - acc: 0.39 - ETA: 15:43 - loss: 1.4919 - acc: 0.39 - ETA: 15:35 - loss: 1.4915 - acc: 0.39 - ETA: 15:28 - loss: 1.4921 - acc: 0.39 - ETA: 15:20 - loss: 1.4922 - acc: 0.39 - ETA: 15:12 - loss: 1.4920 - acc: 0.39 - ETA: 15:05 - loss: 1.4924 - acc: 0.39 - ETA: 14:57 - loss: 1.4922 - acc: 0.39 - ETA: 14:50 - loss: 1.4924 - acc: 0.39 - ETA: 14:42 - loss: 1.4925 - acc: 0.39 - ETA: 14:34 - loss: 1.4930 - acc: 0.39 - ETA: 14:27 - loss: 1.4930 - acc: 0.39 - ETA: 14:19 - loss: 1.4928 - acc: 0.39 - ETA: 14:11 - loss: 1.4930 - acc: 0.39 - ETA: 14:04 - loss: 1.4930 - acc: 0.39 - ETA: 13:56 - loss: 1.4929 - acc: 0.39 - ETA: 13:48 - loss: 1.4928 - acc: 0.39 - ETA: 13:41 - loss: 1.4924 - acc: 0.39 - ETA: 13:33 - loss: 1.4921 - acc: 0.39 - ETA: 13:26 - loss: 1.4923 - acc: 0.39 - ETA: 13:18 - loss: 1.4932 - acc: 0.39 - ETA: 13:10 - loss: 1.4933 - acc: 0.39 - ETA: 13:03 - loss: 1.4933 - acc: 0.39 - ETA: 12:55 - loss: 1.4933 - acc: 0.39 - ETA: 12:48 - loss: 1.4929 - acc: 0.39 - ETA: 12:40 - loss: 1.4928 - acc: 0.39 - ETA: 12:32 - loss: 1.4927 - acc: 0.39 - ETA: 12:25 - loss: 1.4931 - acc: 0.39 - ETA: 12:17 - loss: 1.4933 - acc: 0.39 - ETA: 12:09 - loss: 1.4931 - acc: 0.39 - ETA: 12:02 - loss: 1.4929 - acc: 0.39 - ETA: 11:54 - loss: 1.4931 - acc: 0.39 - ETA: 11:47 - loss: 1.4929 - acc: 0.39 - ETA: 11:39 - loss: 1.4925 - acc: 0.39 - ETA: 11:31 - loss: 1.4923 - acc: 0.39 - ETA: 11:24 - loss: 1.4926 - acc: 0.39 - ETA: 11:16 - loss: 1.4922 - acc: 0.39 - ETA: 11:09 - loss: 1.4919 - acc: 0.39 - ETA: 11:01 - loss: 1.4919 - acc: 0.39 - ETA: 10:53 - loss: 1.4925 - acc: 0.39 - ETA: 10:46 - loss: 1.4929 - acc: 0.39 - ETA: 10:38 - loss: 1.4935 - acc: 0.39 - ETA: 10:31 - loss: 1.4932 - acc: 0.39 - ETA: 10:23 - loss: 1.4929 - acc: 0.39 - ETA: 10:15 - loss: 1.4923 - acc: 0.39 - ETA: 10:08 - loss: 1.4922 - acc: 0.39 - ETA: 10:00 - loss: 1.4923 - acc: 0.39 - ETA: 9:52 - loss: 1.4920 - acc: 0.3974 - ETA: 9:45 - loss: 1.4924 - acc: 0.397 - ETA: 9:37 - loss: 1.4921 - acc: 0.397 - ETA: 9:30 - loss: 1.4914 - acc: 0.398 - ETA: 9:22 - loss: 1.4915 - acc: 0.397 - ETA: 9:14 - loss: 1.4913 - acc: 0.397 - ETA: 9:07 - loss: 1.4911 - acc: 0.398 - ETA: 8:59 - loss: 1.4914 - acc: 0.397 - ETA: 8:52 - loss: 1.4913 - acc: 0.397 - ETA: 8:44 - loss: 1.4914 - acc: 0.397 - ETA: 8:37 - loss: 1.4917 - acc: 0.397 - ETA: 8:29 - loss: 1.4914 - acc: 0.397 - ETA: 8:21 - loss: 1.4915 - acc: 0.397 - ETA: 8:14 - loss: 1.4918 - acc: 0.396 - ETA: 8:06 - loss: 1.4917 - acc: 0.396 - ETA: 7:58 - loss: 1.4917 - acc: 0.396 - ETA: 7:51 - loss: 1.4918 - acc: 0.396 - ETA: 7:43 - loss: 1.4917 - acc: 0.396 - ETA: 7:36 - loss: 1.4917 - acc: 0.395 - ETA: 7:28 - loss: 1.4914 - acc: 0.396 - ETA: 7:20 - loss: 1.4909 - acc: 0.396 - ETA: 7:13 - loss: 1.4910 - acc: 0.396 - ETA: 7:05 - loss: 1.4916 - acc: 0.396 - ETA: 6:58 - loss: 1.4913 - acc: 0.396 - ETA: 6:50 - loss: 1.4909 - acc: 0.397 - ETA: 6:42 - loss: 1.4915 - acc: 0.396 - ETA: 6:35 - loss: 1.4918 - acc: 0.396 - ETA: 6:27 - loss: 1.4918 - acc: 0.396 - ETA: 6:20 - loss: 1.4917 - acc: 0.396 - ETA: 6:12 - loss: 1.4919 - acc: 0.396 - ETA: 6:04 - loss: 1.4915 - acc: 0.397 - ETA: 5:57 - loss: 1.4913 - acc: 0.397 - ETA: 5:49 - loss: 1.4915 - acc: 0.397 - ETA: 5:42 - loss: 1.4922 - acc: 0.396 - ETA: 5:34 - loss: 1.4924 - acc: 0.3959"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646/647 [============================>.] - ETA: 5:26 - loss: 1.4921 - acc: 0.396 - ETA: 5:19 - loss: 1.4918 - acc: 0.396 - ETA: 5:11 - loss: 1.4916 - acc: 0.396 - ETA: 5:03 - loss: 1.4915 - acc: 0.396 - ETA: 4:56 - loss: 1.4921 - acc: 0.396 - ETA: 4:48 - loss: 1.4919 - acc: 0.396 - ETA: 4:41 - loss: 1.4916 - acc: 0.396 - ETA: 4:33 - loss: 1.4916 - acc: 0.396 - ETA: 4:25 - loss: 1.4913 - acc: 0.396 - ETA: 4:18 - loss: 1.4910 - acc: 0.396 - ETA: 4:10 - loss: 1.4909 - acc: 0.396 - ETA: 4:03 - loss: 1.4907 - acc: 0.397 - ETA: 3:55 - loss: 1.4907 - acc: 0.397 - ETA: 3:47 - loss: 1.4907 - acc: 0.396 - ETA: 3:40 - loss: 1.4908 - acc: 0.396 - ETA: 3:32 - loss: 1.4907 - acc: 0.396 - ETA: 3:25 - loss: 1.4907 - acc: 0.396 - ETA: 3:17 - loss: 1.4903 - acc: 0.396 - ETA: 3:09 - loss: 1.4900 - acc: 0.396 - ETA: 3:02 - loss: 1.4899 - acc: 0.396 - ETA: 2:54 - loss: 1.4901 - acc: 0.396 - ETA: 2:47 - loss: 1.4900 - acc: 0.396 - ETA: 2:39 - loss: 1.4900 - acc: 0.396 - ETA: 2:31 - loss: 1.4903 - acc: 0.395 - ETA: 2:24 - loss: 1.4906 - acc: 0.395 - ETA: 2:16 - loss: 1.4909 - acc: 0.395 - ETA: 2:09 - loss: 1.4909 - acc: 0.395 - ETA: 2:01 - loss: 1.4908 - acc: 0.396 - ETA: 1:53 - loss: 1.4904 - acc: 0.396 - ETA: 1:46 - loss: 1.4907 - acc: 0.395 - ETA: 1:38 - loss: 1.4908 - acc: 0.395 - ETA: 1:31 - loss: 1.4904 - acc: 0.395 - ETA: 1:23 - loss: 1.4908 - acc: 0.395 - ETA: 1:15 - loss: 1.4911 - acc: 0.395 - ETA: 1:08 - loss: 1.4909 - acc: 0.395 - ETA: 1:00 - loss: 1.4908 - acc: 0.395 - ETA: 53s - loss: 1.4909 - acc: 0.394 - ETA: 45s - loss: 1.4907 - acc: 0.39 - ETA: 37s - loss: 1.4905 - acc: 0.39 - ETA: 30s - loss: 1.4904 - acc: 0.39 - ETA: 22s - loss: 1.4904 - acc: 0.39 - ETA: 15s - loss: 1.4900 - acc: 0.39 - ETA: 7s - loss: 1.4907 - acc: 0.3955 "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=int(len(X_train)/batchsize), \n",
    "                epochs=6, validation_data=validation_generator, \n",
    "                validation_steps=int(len(X_test)/batchsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_train_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"DR_train_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    probs = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80414534, 0.05421184, 0.12046093, 0.01211276, 0.00906908],\n",
       "       [0.70082736, 0.07930345, 0.17942877, 0.02229862, 0.01814172],\n",
       "       [0.68425024, 0.08667489, 0.18328655, 0.02499642, 0.02079195],\n",
       "       [0.7315191 , 0.07174817, 0.16315447, 0.01880349, 0.01477475],\n",
       "       [0.7310789 , 0.07118486, 0.1679979 , 0.0164353 , 0.01330303],\n",
       "       [0.7377781 , 0.06884215, 0.16138896, 0.01807972, 0.01391109],\n",
       "       [0.6926523 , 0.08778036, 0.17672077, 0.0234363 , 0.01941025],\n",
       "       [0.6910351 , 0.08452048, 0.18147002, 0.02350739, 0.01946706],\n",
       "       [0.6890624 , 0.08359782, 0.18342212, 0.02410621, 0.01981138],\n",
       "       [0.6652998 , 0.09496945, 0.18707496, 0.02858504, 0.02407077]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
